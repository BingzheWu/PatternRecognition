\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\author{Wu Bingzhe\\1200010666\\The school of mathmatical science}
\title{The Pattern Recogition 's homework}
\begin{document}
	\maketitle
	\section{Ex 1}
	
	The log-likelihood function is:
	\begin{equation}
	L(\mu)=\ln p(\mathscr{K}|\mu)=\sum_{i=1}^{N}lnp(x_i|\mu)=\dfrac{1}{2}\sum_{i=1}^{N}(x_i-\mu)^2+C
	\end{equation}
	The derivation of $L(\mu)$ ,we could get that :
	\begin{equation*}
	\dfrac{\partial L(\mu )}{\partial \mu}=\sum_{i=1}^{N}x_i-N\mu =0
	\end{equation*}
	So the maximum likelihood estimation of $\hat{\mu}=\dfrac{1}{N}\sum_{i=1}^{N}x_i$
	
	The bayesian estimation :
	
	First we get the posterior:
	\begin{equation*}
	\begin{split}
	p(\mu|\mathscr{K})&=\dfrac{p(\mathscr{K}|\mu )p(\mu)}{\int p(\mathscr{K}|\mu )p(\mu)du}\\
	&=\alpha \prod_{i=1}^{N}p(x_i|u)p(u)\\
	&=\alpha\prod_{i=1}^{N}\dfrac{1}{\sqrt{2\pi}\sigma}exp[-\dfrac{(x_i-u)^2}{2\sigma^2}]\cdot\sqrt{2\pi}\sigma_0 exp[-\dfrac{(u-u_0)^2}{2\sigma_0^2}] \\
	&=\alpha^{'}[-\dfrac{1}{2}[(\dfrac{N}{\sigma^2}+\dfrac{1}{\sigma_0^2})\mu^2-2(\dfrac{1}{\sigma^2}\sum_{i=1}^{N}x_i+\dfrac{\sigma_0}{\sigma_0^2})\mu ]]
	\end{split}
	\end{equation*}
	According to the formula ,apply the method of undetermined coefficients ,we can get that :
	\begin{equation*}
	\hat{u} =\int\mu p(\mu|\mathscr{K})du=\int\mu\dfrac{1}{\sqrt{2\pi}\sigma_N}exp[-\dfrac{1}{2}(\dfrac{u-u_N}{\sigma_N})^2]du=u_N
	\end{equation*}
	Combine with $\sigma=1,u_0=0,\sigma_0=1$,we can get the bayesian estimation :
	\begin{equation*}
	\hat{\mu}=\dfrac{1}{N+1}\sum_{i=1}^{N}x_i
	\end{equation*}
	\section{Ex 2}
	According to the theory of maximum likelihood estimation and derivation of EX1,
	\begin{equation*}
	l(\theta)=P(x|\theta)=\prod_{k=1}^{n}p(x_k|\theta)
	\end{equation*}
	\begin{equation*}
	H(\theta)=\ln l(\theta)=\sum_{k=1}^{n}\ln p(x_k|\theta)=\sum_{k=1}^{n}\sum_{i=1}^{d}[x_{ki}ln(\theta_i)+(1-x_{ki}\ln(1-\theta_i))]
	\end{equation*}
	In terms of $0=\dfrac{\partial H(\theta)}{\partial \theta_i}=\sum_{k=1}^{n}[\dfrac{x_{ki}}{\theta_i}-\dfrac{1-x_{ki}}{1-\theta_i}]$
	
	We get that:
	\begin{equation*}
	\hat{\theta}=\dfrac{1}{n}\sum_{k=1}^{n}x_{ki},i=1,\cdots,d;
	\end{equation*}
	So,$\hat{\theta}=\dfrac{1}{n}\sum_{k=1}^{n}x_k$
	\section{Ex3}
	According to the theory of bayes estimation,and the derivation of Ex1,the results of multivariate normal distribution is :
    \begin{eqnarray}
    \begin{cases}
    \Sigma_N^{-1}=\Sigma_{0}^{-1}+N\Sigma^{-1}\cr u_N^{T}=(\Sigma^{-1}\sum_{i=1}^{N}x_i^{T}+\mu_0^{T}\Sigma_0^{-1})\Sigma_N
    \end{cases}
    \end{eqnarray}
    In terms of the equation of the matrix ,we have:
    \begin{equation*}
    \begin{split}
    \Sigma_N &=(\Sigma_0^{-1}+(\dfrac{1}{N}\Sigma)^{-1})^{-1}\\
    &=\Sigma_0(\Sigma_0+\dfrac{1}{N}\Sigma)^{-1}\dfrac{1}{N}\Sigma\\
    &=\Sigma_0(N\Sigma_0+\Sigma)^{-1}\Sigma
    \end{split}
    \end{equation*} 
    \begin{equation*}
    \begin{split} 
    \mu_{N}^{T}&=(\Sigma^{-1}\sum_{i=1}^{N}x_i^T+u_0^T\Sigma_0^{-1})\cdot \Sigma_0(N\Sigma_0+\Sigma)^{-1}\Sigma\\
    &=\sum_{i=1}^{N}x_i^T(N\Sigma_0+\Sigma)^{-1}\Sigma_0+\mu_0^T(N\Sigma_0+\Sigma)^{-1}\Sigma\\
    &=\Sigma_0(N\Sigma_0+\Sigma)^{-1}\sum_{i=1}^{N}x_i+\Sigma(N\Sigma_0+\Sigma)^{-1}\mu_0
    \end{split}
    \end{equation*}
\section{Ex4}
\subsection{(1)}
The maximum likelihood estimation of the mean of the normal distribution $\mu$ is $\hat{\mu}=\dfrac{1}{N}\sum_{i=1}^{N}x_i$.So,we have:
\begin{equation*}
    E[\hat{\mu}]=E[\dfrac{1}{N}\sum_{i=1}^{N}]=\dfrac{1}{N}\sum_{i=1}^{N}\mu =\mu
\end{equation*}
So the $\hat{\mu}$ is ubbiased estimation.
\subsection{(2)}
The maximum likelihood estimation of the variance of  the normal distribution $\sigma^2$ is $\hat{\sigma}^2=\dfrac{1}{N}\sum_{i=1}^{N}(x_i-\hat{\mu})^2$.
So:
\begin{equation*}
    \begin{split}
        E[\hat{\sigma}^2]&=E[\dfrac{1}{N}\sum_{i=1}^{N}(x_i-\hat{\mu})^2]\\
                         &=E[\dfrac{1}{N}\sum_{i=1}^{N}x_i^2-\hat{\mu}^2]\\
                         &=\dfrac{1}{N}\sum_{i=1}^{N}(\sigma^2+\mu^2)-(\dfrac{\sigma^2}{N}+\mu^2)\\
                         &=\dfrac{N-1}{N}\sigma^2\neq\sigma^2
    \end{split}
\end{equation*}
So $\hat{\sigma}^2$ is not a ubbiased estimation.
\section{Ex5}
\subsection{(1)}
\begin{equation*}
\begin{split}
\lim\limits_{N\rightarrow\infty}E[\hat{p}_N(x)]&=\lim\limits_{N\rightarrow\infty}E[\dfrac{1}{NV_N}\sum_{i=1}^{N}\phi(\dfrac{x-x_i}{h_N})]\\
&=E[\lim\limits_{N\rightarrow\infty}\sigma_N(x-x_i)]\\
&=\int \delta(x-V)p(V)dv\\
&=p(x)
\end{split}
\end{equation*}
\subsection{(2)}
\begin{equation*}
\begin{split}
\lim\limits_{N\rightarrow \infty}Var[\hat{p}_N(x)]&=\lim\limits_{N\rightarrow \infty}{E\hat{p}^2_N(x)-(E\hat{p}_N(x))^2}\\
&=\lim\limits_{N\rightarrow\infty}E[\dfrac{1}{NV_N}\sum_{i=1}^{N}\phi(\dfrac{x-x_i}{h_N})\dfrac{1}{NV_N}\sum_{j=1}^{N}\phi(\dfrac{x-x_j}{h_N})]
-[\lim\limits_{N\rightarrow\infty}E\hat{p}_N(x)]^2\\
&=\dfrac{1}{N^2}E[\lim\limits_{N\rightarrow\infty}\dfrac{1}{V_N}\sum_{i=1}^{N}\phi(\dfrac{x-x_i}{h_N})\lim\limits_{N\rightarrow\infty}\dfrac{1}{V_N}\sum_{j=1}^{N}
\phi(\dfrac{x-x_j}{h_N})]-p^2(x)\\
&=E[\lim\limits_{N\rightarrow\infty}\delta_N(x-V_1)\delta(x-V_2)]-p^2(x)\\
&=E(\delta(x-V_1)\delta(x-V_2)-p^2(x)\\
&=\int\delta^2(x-V)^2p^2(V)dv-p^2(x)\\
&=p^2(x)-p^2(x)\\
&=0
\end{split}
\end{equation*}
\end{document}
